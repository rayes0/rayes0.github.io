<!doctype html><noscript><style>.jsonly{display:none}</style></noscript><html lang=en-ca><head><meta charset=utf-8><title>A Search for the Best Spaced Learning Algorithm - rayes</title><link rel=stylesheet href=/main.css><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.80.0"></head><body><div class=menu><ul class=menu-ulist><li class=menu-list><a href=https://rayes0.github.io/ class=menu-links>Home</a></li><li class=menu-list><a href=https://rayes0.github.io/blog/ class=menu-links>Writing</a></li><li class=menu-list><a href=https://rayes0.github.io/meta/ class=menu-links>Meta</a></li><li class=menu-list><a href=https://rayes0.github.io/about/ class=menu-links>About</a></li></ul></div><div id=content><div class=page-heading><h1>A Search for the Best Spaced Learning Algorithm</h1></div><div class=container role=main><article class=article class=blog-post><div class=post-meta><span class=meta-post><a style=font-style:normal href=https://rayes0.github.io/categories/psychology/>Psychology</a>&nbsp;
<label class=meta-sep>•</label>
<label>Created: </label><label><label class=slant>Oct 23, 2021</label>
<label class=meta-sep>•</label>
<label>Modified: </label><label class=slant>Apr 6, 2022</label>
<label class=meta-sep>•</label>
<a href=https://rayes0.github.io/about/#post-metadata>Status:</a>&nbsp;
<label class=slant>inprogress</label><div class=blog-tags><a class=tags href=https://rayes0.github.io/tags/productivity/>﹟productivity</a>&nbsp;&nbsp;
<a class=tags href=https://rayes0.github.io/tags/data-science/>﹟data science</a>&nbsp;&nbsp;
<a class=tags href=https://rayes0.github.io/tags/intelligence/>﹟intelligence</a>&nbsp;&nbsp;
<a class=tags href=https://rayes0.github.io/tags/memory/>﹟memory</a>&nbsp;&nbsp;</div></span></div><div class=boxed><div class=boxed-container><div class=boxed-info style=width:7rem><div class=boxed-vertical><label class=boxed-symbol style=font-style:normal>ⓘ</label>
<label class=boxed-label style=font-style:normal>Note: WIP</label></div></div><div class=boxed-text style=margin-left:7rem><label>This post is <a href=/about#post-metadata>flagged as <code>inprogress</code></a> which means I consider it largely incomplete in it's current state. Expect rough, unfinished thoughts.</label></div></div></div><h3 style=font-style:italic>Contents</h3><nav id=TableOfContents><ol><li><a href=#a-quantitative-representation-of-memory>A Quantitative Representation of Memory</a></li><li><a href=#current-implementations>Current Implementations</a><ol><li><a href=#sm2>SM2 and other simple algorithms</a></li><li><a href=#regression-based-on-recall-probability>Regression based on Recall Probability</a></li></ol></li><li><a href=#further-reading>Further Reading</a></li></ol></nav><p>Spaced repetition is an <a href=https://en.wikipedia.org/wiki/Evidence-based_learning>evidence-based learning</a> technique which utilizes the <a href=https://en.wikipedia.org/wiki/Spacing_effect>spacing effect</a>. In contrast to more traditional repetition-based learning strategies such as cramming in which the main focus is on the total number of repetitions, spaced repetition is timing-based and focuses on maximizing efficiency by doing repetitions at scheduled times (typically decided by the difficulty of the material). Numerous studies have been conducted on spaced repetition (and on memory in general), and there is strong evidence suggesting that spaced repetition decreases the number of needed repetitions, and improves retention significantly over the long term. This article attempts to divulge into the specifics of spaced repetition implementations and the existing algorithms implemented in popular software like Anki, SuperMemo, Duolingo, and Quizlet. I will also share some general ideas and observations from the point of view of a daily user of spaced repetition.</p><h1 id=a-quantitative-representation-of-memory><a href=#a-quantitative-representation-of-memory class=hlink>A Quantitative Representation of Memory</a></h1><p>Ebbinghaus (from self experiments) proposed an equation for the <a href=https://en.wikipedia.org/wiki/Forgetting_curve>forgetting curve</a>:</p><p><img src=/ltximg/blog_8ef1da40259646f0eaa5ea2093957011d1747e60.svg alt="$$b =\frac{100k}{(\log(t))^c+k}$$" class=org-svg></p><p>Where <img src=/ltximg/blog_837ee851bb4a529eecf779e3a0ce8f6c72650850.svg alt=$b$ class=org-svg> is the percentage of time saved on relearning (same thing as recall probability), <img src=/ltximg/blog_961a5e4147483cb54e11a466949caf60b50823b2.svg alt=$t$ class=org-svg> is the time in minutes, and <img src=/ltximg/blog_0982722896a2a955589f8281d183de36a52b5188.svg alt=$c$ class=org-svg> and <img src=/ltximg/blog_83ddca1c63a96d4c3dae138f44463ddee089f5ca.svg alt=$k$ class=org-svg> are constants. There are <a href=https://supermemo.guru/wiki/Error_of_Ebbinghaus_forgetting_curve>various reasons</a> to discredit this specific equation, which I will not go into detail on. A quick summary: Ebbinghaus used nonsense syllables to test himself, which had little real world associations and coherence with past memories, and also measured with a comparatively short time interval of around 2 weeks, while spaced repetition is typically implemented well beyond that time period.</p><p>Thus, Ebbinghaus&rsquo;s equation is usually dismissed in favour of one of exponential decay:</p><p><img src=/ltximg/blog_3cfcbf4880462f0c8c7c87e454b830f3e1d14c72.svg alt="$$R(t) = e^{\frac{-t}{S}}$$" class=org-svg></p><p>Where <img src=/ltximg/blog_e69599196f4f075ad59931cf782d2e869c44a710.svg alt=$R(t)$ class=org-svg> is the recall probability as a function of <img src=/ltximg/blog_961a5e4147483cb54e11a466949caf60b50823b2.svg alt=$t$ class=org-svg> (time), and <img src=/ltximg/blog_f495606059dc3da20a34cf42e88afcaee71d5a52.svg alt=$S$ class=org-svg> is the memory stability. The memory stability corresponds to how strong the memory is, specifically how much time it takes for the recall probability to decay to <img src=/ltximg/blog_3b2e5acaa699e3a82883ca5a52041556fc90317a.svg alt=$e^{-1}$ class=org-svg>. This is because when <img src=/ltximg/blog_5974baadad42ce8d5b888da6ad0d2217323d2bf0.svg alt="$t = S$" class=org-svg>, then <img src=/ltximg/blog_ced8fecfdf6c86f0d9ff18c85873464f48eb7113.svg alt="$R(t) = e^{-1}$" class=org-svg>.</p><p>This equation seems to make logical sense. Consider the following cases:</p><ol><li>When <img src=/ltximg/blog_d304922f2842bf682d7cbdcdbdb653de1528221d.svg alt="$t=0$" class=org-svg>, <img src=/ltximg/blog_8a40c2252d90f5ea8d98c45043863651707afae8.svg alt="$R(0) = e^0 = 1$" class=org-svg>. This makes sense because <img src=/ltximg/blog_d304922f2842bf682d7cbdcdbdb653de1528221d.svg alt="$t=0$" class=org-svg> represents the initial retention rate when the item has just been reviewed, which should be 100%.</li><li>When <img src=/ltximg/blog_891ca8e12069a7f933349f70feefe0a33265e07f.svg alt="$t &amp;gt;&amp;gt; S$" class=org-svg> (meaning we have reviewed the item a very long time ago): <img src=/ltximg/blog_e4036bc4fd02f19399f02a89524ec50511800998.svg alt="$R(t) \approx e^{-\infty} \approx 0$" class=org-svg> (we have a retention rate of close to 0%).</li><li>Another idea to consider is that <img src=/ltximg/blog_f495606059dc3da20a34cf42e88afcaee71d5a52.svg alt=$S$ class=org-svg> is not constant between repetitions of the same item. Each time the item is reviewed, <img src=/ltximg/blog_f495606059dc3da20a34cf42e88afcaee71d5a52.svg alt=$S$ class=org-svg> should grow because the item becomes more familiar with each repetition. Moreover, it would make sense for <img src=/ltximg/blog_f495606059dc3da20a34cf42e88afcaee71d5a52.svg alt=$S$ class=org-svg> to increase at an exponential rate because learning should compound over time<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. In other words, as we do more and more repetitions, <img src=/ltximg/blog_6561de32082e5ad1dcbc74486a649d91b8b0121a.svg alt="$S \to \infty$" class=org-svg>, and when <img src=/ltximg/blog_adab122c7eb145f830ed9277a37bee50d1de8517.svg alt="$S &amp;gt;&amp;gt; t$" class=org-svg>, then <img src=/ltximg/blog_5b000321d2b0ebc59507863cb90403a8bf33dd3b.svg alt="$R(t) \approx e^{0} \approx 1$" class=org-svg>. This hypothetically means that after infinite repetitions, our recall probability will be 100%.</li></ol><p>Looking at the equation, this means that in order to accurately predict the recall probability, the main hurdle is finding an accurate value for <img src=/ltximg/blog_f495606059dc3da20a34cf42e88afcaee71d5a52.svg alt=$S$ class=org-svg>. The only other relevant variable is time <img src=/ltximg/blog_961a5e4147483cb54e11a466949caf60b50823b2.svg alt=$t$ class=org-svg>, which is easy to measure.</p><p>The value of <img src=/ltximg/blog_f495606059dc3da20a34cf42e88afcaee71d5a52.svg alt=$S$ class=org-svg> (memory strength) is hard to predict and is influenced by a multitude of factors. Here are some I can think of from the top of my head, ranked from easiest to hardest to measure (from the algorithm&rsquo;s perspective):</p><ul><li>Past reviews and performance on the same item, if the item was seen previously</li><li>Difficulty of the material (subjective to individual differences)</li><li>Format. How the material is presented.</li><li>Previous experience and associations. More experience with similar material in the past will make future review easier. Also the degree of initial learning (<a href=https://faculty.washington.edu/gloftus/Downloads/LoftusForgettingCurves.pdf>Loftus, 1985</a>)</li><li>The current psychological and physical state of the user. Things like emotional disposition (<a href=https://www.sciencedirect.com/science/article/pii/S0022440502001085>Gumora & Arsenio, 2002</a><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>) or consumption of caffiene (<a href=https://link.springer.com/article/10.1007/s11325-014-0976-y>Cole, 2014</a><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>). Sleep (<a href=https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-015-1712-9>Mirghani et al, 2015</a>) especially seems to have large effects on memory strength. Motivation is obviously also a large factor.</li></ul><h1 id=current-implementations><a href=#current-implementations class=hlink>Current Implementations</a></h1><p>Most of the major implementations of scheduling algorithms that I think are relevent.</p><h2 id=sm2><a href=#sm2 class=hlink>SM2 and other simple algorithms</a></h2><p>Used by: <a href=https://faqs.ankiweb.net/what-spaced-repetition-algorithm.html>Anki</a>, <a href=https://mnemosyne-proj.org/help/memory-research>Mnemosyne</a>, <a href=https://knowledge.wanikani.com/wanikani/srs-stages/>Wanikani</a></p><p>Reference: <a href=https://www.supermemo.com/en/archives1990-2015/english/ol/sm2>SM2</a> (P. A.Wozniak, 1998)</p><p>SM2 is probably the most popular implementation of spaced repetition (and certainly the most decorated, despite it&rsquo;s simplicity). In the algorithm, each review item is associated with an ease factor, which we hope to correspond to the difficulty of the review item. The original implementation consists of two initial hard coded learning phase steps: 1 day, and 6 days. Variants have other initial steps, and software like Anki allow customization of these.</p><p>A fairly simple formula is used to calculate subsequent intervals:</p><div class=highlight><pre style=color:#586e75;background-color:#eee8d5;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>New interval = Previous interval * Ease factor
</code></pre></div><p>The ease factor is simply a floating point number between 1.3 and 2.5. It&rsquo;s role, as seen in the formula, is to act as an interval multiplier leading to a larger and larger interval each time. The restrictions of 1.3 through 2.5 attempt to keep the number of repetitions reasonable. An ease below 1.3 will lead to the material being studied too often (usually indicative not because the knowledge itself is too difficult, but because it is poorly formatted or presented), and anything greater than 2.5 will space the intervals too much, especially as they get larger.</p><p>At larger intervals, even 2.5 may be too much, and this will infinitely grow. This is why most software using this provide a top interval cap. Capping at around 10 months to a year sounds like a reasonable interval. Once a review item hits the cap, each review would presumably increase memory utility by a relatively large amount because the time interval is so big, and with hardly any extra time investment, so growing the interval further leads to diminishing returns. In most implementations, the ease factor is adaptive to lowering as well. For example in Anki by default, if a user answers incorrectly, then the ease is decreased by 20%.</p><p>Wanikani uses an alternate version of SM2 with predetermined ease factors and with intervals represented by levelling up of stages, which is less flexible but does the job.</p><h2 id=regression-based-on-recall-probability><a href=#regression-based-on-recall-probability class=hlink>Regression based on Recall Probability</a></h2><p>Used by: <a href=https://medium.com/tech-quizlet/spaced-repetition-for-all-cognitive-science-meets-big-data-in-a-procrastinating-world-59e4d2c8ede1>Quizlet</a><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, <a href=https://aclanthology.org/P16-1174.pdf>Duolingo</a></p><p>The first thought I had when pondering on algorithms for spaced repetition is using a simple logistic regression classifier (trained on the user&rsquo;s previous data) to output a probability that the user will get the review item right given an input of the time interval. We can use this recall probability in numerous ways, Quizlet uses it to order the items for review (items with lower recall probability are reviewed first).</p><p>My thoughts were that the user could choose a desired recall probability (we&rsquo;ll call this the &lsquo;recall threshold&rsquo; for now), which corresponds to the decision threshold of the classifier. A recall threshold of around 80% makes sense (of course, this will vary with different individuals). The algorithm will show the item once the recall threshold for it falls below 80%. In order to accurately predict the recall probability, this classifier would be trained on features based on past couple repetitions of the same item (Quizlet uses the past 3). It would check to see if the prior attempts were correct, how quickly they were answered, and how many consecutive attempts were correct, which will all help predict the classifier predict the probability of answering the current attempt correctly. Of course, the difficulty of the material is a large factor (that Quizlet apparently doesn&rsquo;t use), but difficulty is different for different individuals, which is why other algorithms like the SuperMemo family ask the user to rate how easy the item was to answer and uses that rating when considering further intervals. This does require more cognitive load from the user, which can be ameliorated at the cost of accuracy by having the algorithm automatically assign the rating based on factors like the time taken to answer, or at least provide a suggested score based on such factors.</p><p>Duolingo models its &lsquo;half-life regression&rsquo; approach very similar to the forgetting curve equation described above:</p><p><img src=/ltximg/blog_24691b55ec9a862ac4e7f7e0ea0c10e988d0902e.svg alt="$$p=2^{-\Delta / h}$$" class=org-svg></p><ul><li><img src=/ltximg/blog_5ea3e7592a5e5654b6d57a2338ecada2890dc6a2.svg alt=$p$ class=org-svg> is the probability of recall</li><li><img src=/ltximg/blog_66c40b3a168a2a0488a1a92f5bf9b18f070277f1.svg alt=$\Delta$ class=org-svg> is the time since the item was last reviewed</li><li><img src=/ltximg/blog_cd50c3d6eca9a615acaded8bc5e74aa0387c15f8.svg alt=$h$ class=org-svg> is the <em>half life</em> corresponding to the learner&rsquo;s memory strength. The value of this is hoped to correspond with the amount of time it takes the recall probability to decay to 0.5, or 50%. This is because when <img src=/ltximg/blog_1a63d74823b15114c85aa0000bfd318ce6283891.svg alt="$\Delta = h$" class=org-svg>, the equation evaluates to <img src=/ltximg/blog_174000852186a100a7e6dcaa9e3939b9e9d90256.svg alt="$p = 2^{-1} = 0.5$" class=org-svg>. <img src=/ltximg/blog_cd50c3d6eca9a615acaded8bc5e74aa0387c15f8.svg alt=$h$ class=org-svg> should increase exponentially with every repetition (this is analogous to the <img src=/ltximg/blog_f495606059dc3da20a34cf42e88afcaee71d5a52.svg alt=$S$ class=org-svg> memory strength value), and thus can be modeled by in the form <img src=/ltximg/blog_6cfdd910efa0ec8ef624e1494311bfd4d8dfa751.svg alt=$b^n$ class=org-svg>. Duolingo assumes 2 as the base: <img src=/ltximg/blog_ccae4433d7417ae692a8d0871ec9b0bf8e16457b.svg alt="$$h_{\theta} = 2^{\theta \cdot x}$$" class=org-svg></li></ul><p><img src=/ltximg/blog_4f57cf560218f8f261532577846a228f6e559b38.svg alt=$\theta$ class=org-svg> is a vector of parameters and <img src=/ltximg/blog_8200502396f43963cc03bd777b949929fa9ab630.svg alt=$x$ class=org-svg> is a vector of features. This is now a typical regression problem. All we need is a dataset from previous review performance consisting of the recall rate <img src=/ltximg/blog_5ea3e7592a5e5654b6d57a2338ecada2890dc6a2.svg alt=$p$ class=org-svg>, the time since last review <img src=/ltximg/blog_66c40b3a168a2a0488a1a92f5bf9b18f070277f1.svg alt=$\Delta$ class=org-svg>, and the values for each of the feature vector <img src=/ltximg/blog_8200502396f43963cc03bd777b949929fa9ab630.svg alt=$x$ class=org-svg>. Duolingo used the following features: the total number of times the user has seen the item before, the number of times the item was answered correctly and incorrectly, and a large set of lexeme tags indicating the difficulty of each item. The predicted recall rate can be calculated by using <img src=/ltximg/blog_8650e1e85776ab8ab768e01fc44163770a0898c3.svg alt="$p = 2^{-\Delta / h_{\theta}}$" class=org-svg>, and a loss function can be constructed by comparing the predicted value with the actual measured recall rate from our dataset. We can then find values of <img src=/ltximg/blog_4f57cf560218f8f261532577846a228f6e559b38.svg alt=$\theta$ class=org-svg> that minimize this loss function. I won&rsquo;t go into detail about all of this, check the Duolingo paper linked above for the full equations, apparently they also decided to factor an approximation of <img src=/ltximg/blog_cd50c3d6eca9a615acaded8bc5e74aa0387c15f8.svg alt=$h$ class=org-svg> in the cost function. It seemed to provide decent results, the following image shows an attempt for the algorithm to fit curves for the predicted recall rate based on the forgetting curve from data points (the black &lsquo;x&rsquo; marks):</p><figure><img src=/img/spaced-repetition/duolingo-graph.png alt="(Settles &amp;amp; Mender, 2016)"><figcaption><h4>(Settles & Mender, 2016)</h4></figcaption></figure><h1 id=further-reading><a href=#further-reading class=hlink>Further Reading</a></h1><p>Interesting studies, articles, and links I read (or at least skimmed) when researching the topic.</p><ul><li><a href=https://pcl.sitehost.iu.edu/rgoldsto/courses/dunloskyimprovinglearning.pdf><em>Improving Students' Learning With Effective Learning Techniques: Promising Directions From Cognitive and Educational Psychology</em></a> (J. Dunlosky et al. 2013)</li><li><a href=https://psychclassics.yorku.ca/Ebbinghaus/index.htm><em>Memory: A Contribution to Experimental Psychology</em></a> (H. Ebbinghaus, 1885) Translated from German</li><li>Studies on the forgetting curve, effects of various elements, attempts at quantifying:<ul><li><em>The form of the forgetting curve and the fate of memories</em> (L. Averell and A. Heathcote, 2011) [ <a href=https://sci-hub.se/10.1016/j.jmp.2010.08.009>scihub link</a>]</li><li><em>The Precise Time Course of Retention</em> (D. Rubin, S. Hinton, and A. Wenzel, 1999) [<a href=https://sci-hub.se/10.1037/0278-7393.25.5.1161>scihub link</a>]</li><li><em>Forgetting curves in long-term memory: Evidence for a multistage model of retention</em> (M. Fioravanti and F. Cesare, 1992) [<a href=https://sci-hub.se/10.1016/0278-2626(92)90073-U>scihub link</a>]</li><li><a href=https://faculty.washington.edu/gloftus/Downloads/LoftusForgettingCurves.pdf><em>Evaluating Forgetting Curves</em></a> (G. R. Loftus, 1985)</li><li>Jost&rsquo;s Law, cited in <a href=http://wixtedlab.ucsd.edu/publications/wixted/Jost_Law.pdf>another paper on retrograde amnesia</a> (J. T. Wixted, 2004)</li></ul></li></ul><p><strong>Implementations</strong></p><ul><li><p><a href=https://faqs.ankiweb.net/what-spaced-repetition-algorithm.html>Anki&rsquo;s algorithm</a></p></li><li><p>Duolingo&rsquo;s algorithm: <a href=https://aclanthology.org/P16-1174.pdf><em>A Trainable Spaced Repetition Model for Language Learning</em></a> (B. Settles & B. Mender, 2016) [<a href=https://github.com/duolingo/halflife-regression>Github</a>]</p></li><li><p><a href=https://github.com/fasiha/ebisu>Ebisu</a>: <a href=https://github.com/fasiha/ebisu>Github</a></p></li><li><p><a href=https://memrise.zendesk.com/hc/en-us/articles/360015889057-How-does-the-spaced-repetition-system-work->Memrise&rsquo;s algorithm</a></p></li><li><p><a href=https://medium.com/tech-quizlet/spaced-repetition-for-all-cognitive-science-meets-big-data-in-a-procrastinating-world-59e4d2c8ede1>Quizlet&rsquo;s algorithm</a></p></li><li><p>SuperMemo-based (only major versions):</p><ul><li>SM-0</li><li>SM-2 (1987)</li><li><a href=https://supermemo.guru/wiki/First_fast-converging_spaced_repetition_algorithm:_Algorithm_SM-5>SM-5</a> (1989)</li><li><a href=https://supermemo.guru/wiki/First_data-driven_spaced_repetition_algorithm:_Algorithm_SM-8>SM-8</a> (1995)</li><li><a href=https://supermemo.guru/wiki/Algorithm_SM-15>SM-15</a> (2013)</li><li><a href=https://supermemo.guru/wiki/Algorithm_SM-17>SM-17</a> (2016)</li><li><a href=https://supermemo.guru/wiki/Algorithm_SM-18>SM-18</a> (2020)</li><li>Other SuperMemo resources:<ul><li><a href=https://supermemo.guru/wiki/History_of_incremental_reading>Incremental Reading</a> (2000)</li><li><a href=https://www.supermemo.com/en/articles/history>Comprehensive history of SuperMemo</a></li><li><a href=https://supermemo.guru/wiki/History_of_spaced_repetition>SuperMemo&rsquo;s history of spaced repetition</a></li></ul></li></ul></li><li><p><a href=https://knowledge.wanikani.com/wanikani/srs-stages/>Wanikani&rsquo;s algorithm</a></p></li></ul><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Learning compounds over time, or at least should, in theory. This is because at the beginning stages of learning something unfamiliar, an individual has no pre-existing <a href=https://en.wikipedia.org/wiki/Schema_(psychology)>schema</a> of the concept. As they spend time with the material, they will develop some sort of a schema for the concept, which will help them better understand related concepts in the future. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p><a href=https://sci-hub.se/10.1016/S0022-4405(02)00108-5>scihub link</a> (G. Gumora & W. F. Arsenio, 2002, <em>Emotionality, Emotion Regulation, and School Performance in Middle School Children</em>) <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p><a href=https://sci-hub.se/10.1007/s11325-014-0976-y>scihub link</a> (J. S. Cole, 2014, <em>A survey of college-bound high school graduates regarding circadian preference, caffeine use, and academic performance</em>) <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Quizlet doesn&rsquo;t implement true spaced repetition because it&rsquo;s designed for cramming. It doesn&rsquo;t have a strict scheduler and focuses primarily on the order of review, with higher priority items (ones with lower recall probability) being shown first. Quizlet does have a &lsquo;Long-Term Learning&rsquo; mode with a scheduler that appears to follow a fixed formula of <code>new interval = (old interval * 2) + 1</code> with new items starting at a 1 day interval. Items answered incorrectly are reset to the same status as new items. Of course, this is less than optimal as the fixed rate multiplier assumes that all material is the same difficulty. <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section><style>@media(min-width:600px){article>p:first-of-type{max-width:85%;margin-left:auto;margin-right:auto}}</style><style>article>p:first-of-type{margin-top:2em;text-align:justify}article>p:first-of-type::first-letter{font-weight:100;font-size:4rem;float:left;padding-right:6px}</style></article></div></div><footer><div class=container><p class=credits><a href=https://rayes0.github.io/blog/index.xml>rss</a>
•
<a href=https://github.com/rayes0/blog/>source</a></p><p class="credits powered-by">powered by:&nbsp;
<a href=https://gohugo.io/>Hugo</a>&nbsp;
<a href=https://ox-hugo.scripter.co/>ox-hugo</a>&nbsp;
<a href=https://orgmode.org/>Org mode</a>&nbsp;</p><p class="credits copyright"><a href=https://rayes0.github.io/about>rayes</a>
&copy;
2022
•
<a href=https://creativecommons.org/licenses/by/4.0/>license</a></p></div></footer></body></html>