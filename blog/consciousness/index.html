<!doctype html><noscript><style>.jsonly{display:none}</style></noscript><html lang=en-ca><head><meta charset=utf-8><title>Why Artificial Consciousness may be possible - rayes</title><link rel=stylesheet href=/main.css><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.80.0"></head><body><div class=menu><ul class=menu-ulist><li class=menu-list><a href=https://rayes0.github.io/ class=menu-links>Home</a></li><li class=menu-list><a href=https://rayes0.github.io/blog/ class=menu-links>Posts</a></li><li class=menu-list><a href=https://rayes0.github.io/meta/ class=menu-links>Meta</a></li><li class=menu-list><a href=https://rayes0.github.io/about/ class=menu-links>About</a></li></ul></div><div id=content><div class=page-heading><h1>Why Artificial Consciousness may be possible</h1></div><div class=container role=main><article class=article class=blog-post><div class=post-meta><span class=meta-post><a style=font-style:normal href=https://rayes0.github.io/categories/ai/>AI</a>&nbsp;
<label class=meta-sep>•</label>
<label>Created: </label><label><label class=slant>Jul 30, 2021</label>
<label class=meta-sep>•</label>
<label>Modified: </label><label class=slant>Feb 24, 2022</label>
<label class=meta-sep>•</label>
<a href=https://rayes0.github.io/about/#post-metadata>Status:</a>&nbsp;
<label class=slant>inprogress</label><div class=blog-tags><a class=tags href=https://rayes0.github.io/tags/philosophy/>﹟philosophy</a>&nbsp;&nbsp;
<a class=tags href=https://rayes0.github.io/tags/psychology/>﹟psychology</a>&nbsp;&nbsp;
<a class=tags href=https://rayes0.github.io/tags/intelligence/>﹟intelligence</a>&nbsp;&nbsp;</div></span></div><div class=boxed><div class=boxed-container><div class=boxed-info style=width:7rem><label class=boxed-symbol>ⓘ</label>
<label class=boxed-label>Note: WIP</label></div><div class=boxed-text style=margin-left:7rem><label>This post is <a href=/about#post-metadata>flagged as <code>inprogress</code></a> which means I consider it largely incomplete and will definitely add to it in the future.</label></div></div></div><h3 style=font-style:italic>Contents</h3><nav id=TableOfContents><ol><li><a href=#consciousness-is-physical>Consciousness is Physical</a></li><li><a href=#can-an-airplane-fly>Can an airplane fly?</a></li><li><a href=#implications-and-counterarguments>Implications and Counterarguments</a><ol><li><a href=#chinese-rooms-can-understand>Chinese Rooms can understand</a></li><li><a href=#does-this-mean-all-systems-of-enough-complexity-are-conscious>Does this mean all systems of enough complexity are conscious?</a></li></ol></li><li><a href=#virtual-minds>Virtual Minds</a></li></ol></nav><p>It is commonly accepted that machines are not and cannot be conscious, and that any perceived consciousness is merely an imitation of organic consciousness. While I don&rsquo;t disagree with these claims, there is not much solid proof for them, though of course, the same is true for the converse. However, it is still worth considering both sides of the problem. Therefore, for arguments sake, this article will lay out a series of examples and thought experiments potentially proving that artificial conscious <em>could be</em> possible. &lsquo;Could be&rsquo; because I am of the opinion that, as of right now, consciousness cannot be concretely proved <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, and may never be unless we find a reliable way to functionally link two brains (perhaps with a neurostimulator similar to transcranial ultrasound<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>) and convey first-hand experience in real time. However, even if we are able to do this and find out that machines are in fact, not conscious, I describe some reasons why we should treat them as conscious anyways. Keep in mind that this is all for speculation and interest sake. Don&rsquo;t take it too seriously. I personally do not think that machines can be conscious. At the end of the article, I will explain why.</p><h1 id=consciousness-is-physical>Consciousness is Physical</h1><p>Consciousness is a physical property. This is contrary to what many perceive intuitively. It is instinctive to think that there is something special metaphysically in objects that are conscious which make them conscious. Another factor promoting this manner of thought may simply be our ego. We aren&rsquo;t willing to accept that we are purely physical beings.</p><p>However, from what we can currently observe:</p><ul><li>If the physical state of the brain is changed, our consciousness appears to be altered correlationally (Eg: brain injuries like accident-induced amnesia, experiences with drugs, etc.)</li><li>If we physically alter the brain beyond a certain level of repair, or if blood stops flowing to the brain, we lose consciousness.</li><li>Larger and more physically advanced brains result with what seems to be higher levels of consciousness. However, this may just be the result of increased cognitive capabilities in areas like problem solving and a wider range of emotional expression. As will be noted later, <a href=#can-an-airplane-fly>intelligence is as prerequisite for consciousness</a>, so this is not a problem.</li></ul><p>As such, it seems that consciousness can be explained basically entirely from a physical point of view. This matches what we see in real life. When was the last time you thought a pen was conscious? Probably never. When was the last time you thought a dog was conscious? Probably every time you see one. What&rsquo;s the difference between a pen and a dog? The most obvious is physical differences. You could say that there is something outside the physical realm (like a spirit common in religion) which the dog has that makes it different from a pen. However, following Occam&rsquo;s Razor, if the difference seems to be amply explained by physical means, why resort to adding an extra layer?</p><p>Because consciousness can be explained through physical means, and we can observe that some objects are conscious whereas others are not, then there must be physical constraints allowing certain experiences. In other words, there are some set of limitations on physical systems for consciousness to exist.</p><h1 id=can-an-airplane-fly>Can an airplane fly?</h1><p>Consciousness and intelligence are connected. This is not an intuitive idea, mainly due to the fact that intelligence is often perceived as a functional, non-theoretical feature and consciousness as a theoretical one. My ideas in this section are based partly on the <a href=https://en.wikipedia.org/wiki/Integrated_Information_Theory>IIT theory</a>, which I find particularly convincing in this area, and partly some other observations. In order to explain this further, we need to clarify a few things about information, the backbone of intelligence, and draw a connection between it and consciousness:</p><ul><li><a href=https://en.wikipedia.org/wiki/Information_theory#Entropy_of_an_information_source>Information is the reduction of entropy</a> due to <a href=https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence>conservation of expected evidence</a>. If I know one expression evaluates to one and only one thing (eg: &ldquo;one plus one is two&rdquo;), it follows that all other possibilities are false.</li><li>Intelligence results simply from the flow of information in a meaningful way. What I mean by &ldquo;meaningful way&rdquo; is that it creates some sort of a cause and effect. Any system that does not produce a cause and effect on itself or other systems cannot be intelligent, and in fact we can ignore these systems entirely, because they do literally nothing functionally and for our purposes may as well not be there (and according to functionalists, they don&rsquo;t even exist). By this definition, every object that exists and has mass is intelligent to some degree, because by simply having mass they are creating a warp in spacetime described by Einstein&rsquo;s general relativity, which is a case of a cause and effect relationship.</li><li>Using this, we can define a baseline for intelligence. Consider a hypothetical object with some mass that just sits there, creating nothing more than a curvature in spacetime. This object would be the least intelligent object we can ever get because it creates the least cause and effect reduction of entropies, so we could assign it an absolute intelligence of 0. Compare this to something more intelligent, like a modern computer. By performing calculations through millions of cases of cause and effect, it is significantly more intelligent than our simple object. We could create mathematical representations for these, and this has already <a href=https://en.wikipedia.org/wiki/Integrated_Information_Theory#Mathematics:_formalization_of_the_postulates>been done by others</a>.</li><li>When we as conscious entities perceive something through an input of information consciously (I will call this an &lsquo;experience&rsquo;), we are distinguishing this state from a theoretically infinite number of other possible states. What we consider non-conscious entities can only distinguish between a lesser, usually finite number of possible states (eg: A light bulb can only distinguish between two states, an &lsquo;on&rsquo; state where electrons are flowing through it, and an &lsquo;off&rsquo; state where electrons are not flowing through it). However, the ability to distinguish between an infinite number of possible states is not consciousness itself, rather it is when this information flow is integrated with other information flows (see the next point).</li><li>Consciousness emerges as a result of this flow of information interacting with the appropriate physical system (see <a href=#consciousness-is-physical>Consciousness is Physical</a>). The appropriate physical system is one where the sub-elements of this experience are interdependent from each other and integrated in a unified way. Eg: Take for example the experience of seeing an object with your eyes. Various elements of the object are taken into account at once, such as the shape, colour, texture, etc. These are inseparable from each other. You can&rsquo;t see something and notice <em>only</em> it&rsquo;s shape, without any care about colour. It&rsquo;s impossible to. We have evidence for this as an indicator of consciousness because non-conscious entities do not have this integration of experience. If I take a picture with a camera, I can easily permanently remove the picture by deleting it from the memory card. I can edit the pixels to change the image to grayscale, hence removing one aspect of the information flow independent from other aspects. Contrast this to me seeing some scene with my eyes. I can&rsquo;t easily delete that one experience from my brain, or separate certain flows of information without effecting other experiences. This is because that scene has been integrated and intertwined in the physical structure that is my mind.</li></ul><p>If we assume these to be true, then a variety of implications follow:</p><ul><li>We can&rsquo;t reduce consciousness to individual components because all experiences are together in one unified whole. (Thus structuralism cannot be true.)</li><li>Intelligence is a prerequisite for consciousness. This is not a problem for us (at least currently) because as we have established, anything that has mass is intelligent to some degree. And when we are talking about computers being conscious or not, they all have mass.</li><li><em>This theory neither proves nor disproves the existence of a spirit or G(g)od.</em> Some entity like a spirit may have no mass in our dimensions, but they can &lsquo;exist&rsquo; and be intelligent from our perspective if they can create casual relationships which affect our dimensions (which is claimed to happen in most religions). A hypothetical entity fitting this description could be conscious if they create the equivalent of what our minds do through the integration of information flow in whatever dimensions they are governed by. As touched on previously, Occam&rsquo;s razor discourages the existence of such entities, but doesn&rsquo;t do anything to prove they don&rsquo;t or can&rsquo;t exist.</li><li>Dropping in and out of consciousness, or dropping to lower levels of consciousness is possible and occurs frequently. A lower level of consciousness can result from two things: (1) The brain doing less integration of information flows, such as when someone is under the influence of certain drugs which impair brain activity. (2) The total information input is less, resulting in less information flow and which ultimately results in less integration. An example of this would be when sleeping.</li><li>In a conscious entity, thoughts are irreversible, they can&rsquo;t be deleted or cleanly overwritten, because once a certain experience is integrated with other ones, the state of the entire system can&rsquo;t be reversed. You may argue that forgetting something is a case against this because it seemingly deletes select thoughts cleanly. I argue that you can&rsquo;t ever <em>completely</em> forget something. What we call &lsquo;forgetting&rsquo; is really just the term for when a though has faded to the point that it can&rsquo;t be recalled from your consciousness. It still exists, and just can&rsquo;t be accessed, explaining <a href=https://en.wikipedia.org/wiki/Exceptional_memory>numerous memory phenomena</a>.</li></ul><h1 id=implications-and-counterarguments>Implications and Counterarguments</h1><h2 id=chinese-rooms-can-understand>Chinese Rooms can understand</h2><blockquote><p>The whole is greater than the parts.</p><p>(Aristotle)</p></blockquote><p>John Searle&rsquo;s Chinese room thought experiment is commonly used to debunk artificial consciousness. The experiment goes like this: Suppose a non-Chinese speaker is in a room with a reference book that enables them to respond to any Chinese phrase into English and vice versa, at the level of human intelligence (in other words, it passes the <a href=https://en.wikipedia.org/wiki/Turing_test>Turing test</a>). Another Chinese-speaking individual outside the room slips a note written in Chinese through a slot in the door. The non-Chinese speaker then uses the book to create a reply and returns it to the individual outside, thereby convincing the individual that there is a Chinese speaker in the room.</p><p>Searle proposed that because the individual inside the room doesn&rsquo;t actually understand Chinese, the individual and room only create the illusion of understanding. Therefore, a strong AI may create the illusion of consciousness but is in reality just a program following instructions, thus artificial consciousness is impossible because no matter how intelligent an AI program is, it will always be, in reality, a Chinese room where the computer is merely following code.</p><p>One common argument against this is treating the contents to the room and the man inside as a system, and although the man himself may not understand Chinese, the man-room system as a whole does. The man is merely a utility in executing out the instructions from the book, and when the book is combined with the man, the system can &ldquo;understand&rdquo; Chinese. Searle replied to this by proposing the fact that if the man memorizes the handbook and does the exact same process, but in his head? He proposed that by linking the computation in one area, there is then no system, and the man himself still doesn&rsquo;t understand Chinese. This argument proves very little as it is still true that regardless of where the computation is taking place, the system as a whole still understands Chinese. Searle&rsquo;s argument also doesn&rsquo;t take into account the fact that the man himself has no need to understand Chinese for the entire system to be able to do so. This is much like how the atoms that make up a biological brain don&rsquo;t need to be conscious for the brain as a whole to be.</p><p>What the Chinese room argument ignores is the fact that <em>any</em> algorithm causing behaviour and intelligence (and thus understanding), no matter how seemingly complex it is in the human mind, can be broken down into a network of fundamental AND and OR logic gates (which is incidentally the exact idea that deep learning methods like neural networks are built from<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>). Technically, if you wanted to, you could write a bunch of if-else statements to model a brain. Sufficiently complex interactions between parts can produce properties the parts don&rsquo;t have alone (see <a href=https://en.wikipedia.org/wiki/Emergence>here</a> and <a href=https://en.wikipedia.org/wiki/Spontaneous_order>here</a>). There is no reason to believe that this doesn&rsquo;t extend to neurobiological processes. In other words, a sufficiently complex system (artificial or biological) can produce conscious phenomena. The Chinese room argument assumes that the room can never understand Chinese because consciousness is not emergent, that it can&rsquo;t be broken down into simple parts, when really consciousness emerges from a system of enough complexity. A system complex enough that it is able to pass the Turing test and achieve human level intelligence will already need to be as complex as the human brain itself. Given this complexity, it is possible that consciousness and understanding can emerge from an artificial Chinese room system.</p><h2 id=does-this-mean-all-systems-of-enough-complexity-are-conscious>Does this mean all systems of enough complexity are conscious?</h2><p>No. As we have talked about, because <a href=#consciousness-is-physical>consciousness is physical</a>, there must also be physical constraints. Certain systems cannot be conscious no matter what level of complexity they become, for example, the example given earlier of a camera taking a photo. No matter how big the photo becomes, or how much bits of information flows through the camera, it does not <a href=#can-an-airplane-fly>integrate this</a> in a meaningful way to create a conscious experience.</p><h1 id=virtual-minds>Virtual Minds</h1><p>The possibility of qualia being accessible to us in terms of physical structure is not zero. Physicists had previously had no idea how magnetism and other <a href=https://en.wikipedia.org/wiki/Action_at_a_distance>action at a distance forces</a> were possible. After all, how could an object exert a force on another object without touching it? The discovery of breakthroughs like <a href=https://en.wikipedia.org/wiki/Force_carrier>messenger particles</a> and <a href=https://en.wikipedia.org/wiki/Quantum_entanglement>quantum entanglement</a> solved these and enabled us to represent these previously thought to be magical forces through physical and structural means (though gravity is still somewhat a mystery). Perhaps the same could be true for qualia, perhaps there is a mediating particle for experience (an thus also thought, because thinking is the experience of consciousness), where we can also analyze the metaphysical realm of consciousness through known theorems and laws from the chemical and physical sciences.</p><p>If this is were true, this would entail a whole world of possibilities for us. In addition to AI-related advances, if we are capable of a physical representation of consciousness, this would give us means to perhaps store streams of consciousness (AKA thoughts) for later retrieval and perhaps &lsquo;play&rsquo; it back with another mind. It could potentially allow us to artificially incur thoughts on a conscious entity. This is all mere speculation and most likely not possible, certainly not currently with our technology, but it is an interesting to speculate about.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>This depends on what consciousness actually entails, a few of which are discussed in this article. If we define consciousness as a first-person experience, then we cannot prove if anything apart from ourselves is conscious, because we cannot (at least currently) experience what another consious entity is experiencing. Additionally, consciousness is a spectrum rather than a discrete yes-or-no classification, and has an indeterminate area in the middle in which makes it difficult to discern a threshold for a decision boundary. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>We have already succeeded in creating somewhat of a information link between two brains through the use of tFUS (transcranial focused ultrasound), where a human&rsquo;s intentions measured electroencephalographicly by measuring SSVEP signal intensity with a computer. The computer emitted tFUS signals to the rat, which stimulated it to elicit tial movement according to when the human intended it to.<br>(Yoo, S.-S., Kim, H., Filandrianos, E., Taghados, S. J., & Park, S. (2013). Non-Invasive Brain-to-Brain Interface (BBI): Establishing Functional Links between Two Brains. PLoS ONE, 8(4), e60410. <a href=https://doi.org/10.1371/journal.pone.0060410>https://doi.org/10.1371/journal.pone.0060410</a>) <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>The AND and OR basic logic gates are fairly straightforward to implement, and can be done with just two layers of artificial neurons. Adding just one hidden layer allows us to fit more complex computational models like XOR and XNOR. When we move past 4-5 layers, with each hidden layer having a sufficient amount of neurons, extremely complex logic can be modeled. This is all done simply by the stacking and chaining of simple logic (the AND, OR, NOT, etc. basic logic gates). When we backpropogate a neural network during the training phases, it would be a near impossible task to track all the possible routes through it. This is why these networks are often likened to a &lsquo;black box&rsquo;, because we don&rsquo;t actually know much of whats going on inside. Much like how we can create virtual machines, virtual communities, and virtual reality, neural networks may very well be virtual minds. <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section><style>@media(min-width:600px){article>p:first-of-type{max-width:85%;margin-left:auto;margin-right:auto}}</style><style>article>p:first-of-type{margin-top:2em;text-align:justify}article>p:first-of-type::first-letter{font-weight:100;font-size:4rem;float:left;padding-right:6px}</style></article></div></div><footer><div class=container><p class=credits><a href=https://rayes0.github.io/blog/index.xml>rss</a>
•
<a href=https://github.com/rayes0/blog/>source</a></p><p class="credits powered-by">powered by:&nbsp;
<a href=https://gohugo.io/>Hugo</a>&nbsp;
<a href=https://ox-hugo.scripter.co/>ox-hugo</a>&nbsp;
<a href=https://orgmode.org/>Org mode</a>&nbsp;</p><p class="credits copyright"><a href=https://rayes0.github.io/about>rayes</a>
&copy;
2022
•
<a href=https://creativecommons.org/licenses/by/4.0/>license</a></p></div></footer></body></html>